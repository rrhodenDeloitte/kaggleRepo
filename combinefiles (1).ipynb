{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":16431,"databundleVersionId":824140,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-27T03:33:24.434815Z","iopub.execute_input":"2025-09-27T03:33:24.435248Z","iopub.status.idle":"2025-09-27T03:33:24.442347Z","shell.execute_reply.started":"2025-09-27T03:33:24.435216Z","shell.execute_reply":"2025-09-27T03:33:24.441233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json, time, re\nfrom pathlib import Path\nfrom typing import Tuple, Iterable\nimport requests\nfrom tqdm import tqdm\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# ===================== CONFIG =====================\nTAXI_TYPES: Iterable[str] = (\"yellow\", \"green\")   \nSTART_YM: Tuple[int,int] = (2022, 7)              \nEND_YM:   Tuple[int,int] = (2025, 6)              \n\nOUT_DIR = Path(\"nyc-taxi-jul2022-jun2025\")\nKAGGLE_OWNER = \"ronakal\"             \nKAGGLE_SLUG  = \"nyc-tlc-trip-records-jul2022-jun2025\"  \n\n# TLC official file pattern (no scraping required)\nURL_TMPL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi}_tripdata_{year}-{month:02d}.parquet\"\n# ==================================================\n\ndef ym_iter(y0, m0, y1, m1):\n    y, m = y0, m0\n    while (y < y1) or (y == y1 and m <= m1):\n        yield y, m\n        m += 1\n        if m == 13: y, m = y + 1, 1\n\ndef normalize(df: pd.DataFrame, taxi: str, y: int, m: int) -> pd.DataFrame:\n    ren = {}\n    if \"tpep_pickup_datetime\" in df.columns:  ren[\"tpep_pickup_datetime\"]  = \"pickup_datetime\"\n    if \"lpep_pickup_datetime\" in df.columns:  ren[\"lpep_pickup_datetime\"]  = \"pickup_datetime\"\n    if \"tpep_dropoff_datetime\" in df.columns: ren[\"tpep_dropoff_datetime\"] = \"dropoff_datetime\"\n    if \"lpep_dropoff_datetime\" in df.columns: ren[\"lpep_dropoff_datetime\"] = \"dropoff_datetime\"\n    if ren: df = df.rename(columns=ren)\n    df[\"taxi_type\"] = taxi\n    df[\"year\"] = y\n    df[\"month\"] = m\n    return df\n\ndef download(url: str, dest: Path, retries=3, timeout=120):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    for k in range(retries):\n        try:\n            with requests.get(url, stream=True, timeout=timeout) as r:\n                r.raise_for_status()\n                total = int(r.headers.get(\"content-length\", 0))\n                with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n                    for chunk in r.iter_content(1024*512):\n                        if chunk:\n                            f.write(chunk)\n                            pbar.update(len(chunk))\n            if dest.stat().st_size < 1024:\n                raise RuntimeError(\"Downloaded file suspiciously small\")\n            return True\n        except Exception as e:\n            if k == retries - 1:\n                print(f\"  -> giving up: {e}\")\n                return False\n            time.sleep(1 + k)\n\ndef build():\n    raw = OUT_DIR / \"raw\"\n    raw.mkdir(parents=True, exist_ok=True)\n\n    # 1) Download monthlies\n    files = []\n    for y, m in ym_iter(*START_YM, *END_YM):\n        for taxi in TAXI_TYPES:\n            url = URL_TMPL.format(taxi=taxi, year=y, month=m)\n            dest = raw / f\"{taxi}_tripdata_{y}-{m:02d}.parquet\"\n            if not dest.exists():\n                print(f\"Downloading {dest.name}\")\n                ok = download(url, dest)\n                if not ok:\n                    continue\n            files.append((dest, taxi, y, m))\n\n    if not files:\n        raise SystemExit(\"No files downloaded—check TAXI_TYPES and date window.\")\n\n    # 2) Build a unified schema (single pass)\n    unified = None\n    for p, taxi, y, m in files:\n        try:\n            df = pd.read_parquet(p, engine=\"pyarrow\")\n            df = normalize(df, taxi, y, m)\n            tbl = pa.Table.from_pandas(df, preserve_index=False)\n            unified = tbl.schema if unified is None else pa.unify_schemas([unified, tbl.schema])\n        except Exception as e:\n            print(f\"[schema] skip {p.name}: {e}\")\n\n    if unified is None:\n        raise SystemExit(\"Failed to infer unified schema.\")\n\n    # 3) Write per-year combined Parquet (memory-safe append)\n    by_year = {}\n    for p, taxi, y, m in files:\n        by_year.setdefault(y, []).append((p, taxi, m))\n    for y, items in sorted(by_year.items()):\n        # keep only months within window (handles 2022 and 2025 partials)\n        items = [(p,t,m) for (p,t,m) in items if (y, m) >= START_YM and (y, m) <= END_YM]\n        if not items: continue\n        outp = OUT_DIR / f\"nyc_taxi_{y}.parquet\"\n        writer = None\n        try:\n            for p, taxi, m in sorted(items, key=lambda x: x[2]):\n                try:\n                    df = pd.read_parquet(p, engine=\"pyarrow\")\n                    df = normalize(df, taxi, y, m)\n                    tbl = pa.Table.from_pandas(df, preserve_index=False)\n                    # Add any missing columns as nulls + cast to unified types where possible\n                    cols = []\n                    names = set(tbl.schema.names)\n                    for f in unified:\n                        if f.name in names:\n                            col = tbl[f.name]\n                            if not col.type.equals(f.type):\n                                try: col = pa.compute.cast(col, f.type)\n                                except Exception: pass\n                            cols.append(col)\n                        else:\n                            cols.append(pa.nulls(len(tbl)).cast(f.type, safe=False))\n                    tbl = pa.Table.from_arrays(cols, schema=unified)\n                    if writer is None:\n                        writer = pq.ParquetWriter(outp, unified, compression=\"snappy\")\n                    writer.write_table(tbl)\n                except Exception as e:\n                    print(f\"[write {y}] skip {p.name}: {e}\")\n        finally:\n            if writer: writer.close()\n        print(f\"✔ wrote {outp}\")\n\n    # 4) Kaggle metadata\n    meta = {\n        \"title\": \"NYC TLC Trip Records (Jul 2022–Jun 2025, official Parquet)\",\n        \"id\": f\"{KAGGLE_OWNER}/{KAGGLE_SLUG}\",\n        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n        \"description\": (\n            \"Official monthly Parquet files from NYC TLC, filtered to Jul 2022–Jun 2025. \"\n            \"Light normalization: pickup/dropoff datetime harmonized; taxi_type/year/month added. \"\n            \"Source: NYC TLC Trip Record Data.\"\n        ),\n        \"tags\": [\n            {\"name\": \"transportation\"},\n            {\"name\": \"new-york\"},\n            {\"name\": \"public-datasets\"},\n            {\"name\": \"parquet\"},\n            {\"name\": \"timeseries\"}\n        ],\n    }\n    (OUT_DIR / \"dataset-metadata.json\").write_text(json.dumps(meta, indent=2))\n    print(\"✔ prepared dataset-metadata.json\")\n\nif __name__ == \"__main__\":\n    OUT_DIR.mkdir(parents=True, exist_ok=True)\n    build()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T04:22:13.827875Z","iopub.execute_input":"2025-09-27T04:22:13.828156Z","iopub.status.idle":"2025-09-27T04:25:19.922983Z","shell.execute_reply.started":"2025-09-27T04:22:13.828133Z","shell.execute_reply":"2025-09-27T04:25:19.921749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.getcwd())  # shows current directory\n!ls -lh\n!ls -lh nyc-taxi-jul2022-jun2025","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working\n!ls -lh /kaggle/working/nyc-taxi-jul2022-jun2025","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T04:47:23.340468Z","iopub.execute_input":"2025-09-27T04:47:23.341384Z","iopub.status.idle":"2025-09-27T04:47:23.687943Z","shell.execute_reply.started":"2025-09-27T04:47:23.341349Z","shell.execute_reply":"2025-09-27T04:47:23.686603Z"}},"outputs":[],"execution_count":null}]}